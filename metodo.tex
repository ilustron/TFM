\chapter{Simulaciones numéricas}

\section{Modelo discreto}

Para la realización de simulaciones numéricas es necesario discretizar la
membrana cristalina. Para ello, vamos a considerar una red 
discreta de $N$ nodos (figura~\ref{red_rombo}), cuyas conexiones o enlaces
forman una red regular triangular. El contorno de esta red es un rombo de lado
$L=\sqrt{N}$, en este sentido podemos calificar la red como cuadrada. 
Las condiciones en la frontera son libres, lo que significa que el índice de
coordinación, el número de primeros vecinos\footnote{Entendemos por primeros
  vecinos de un nodo, aquellos nodos que tienen un sólo enlace con él.} de un
determinado nodo, es menor en el contorno. Esta falta de homogeneidad,
son los llamados efectos de borde, que producen que los valores 
de los observables en el interior son diferentes que en el contorno. Puesto
que la cantidad de nodos interiores de la membrana es proporcional a $L^2$, y
el número de nodos del contorno a $L$, es de esperar que los efectos de borde
disminuyan a medida que aumenta el tamaño del sistema hasta hacerse
despreciables. Aún así, la forma óptima del contorno de una
membrana para minimizar los efectos de borde es el hexágono, donde por
ejemplo, ningún nodo tiene un nodo tiene un índice de coordinación menor que
3, en nuestro caso, con el contorno romboidal el menor número de coordinación
es 2. Aunque el contorno romboidal tenga el inconveniente de mayores efectos
de borde, lo hemos elegido principalmente por poder comparar nuestras medidas
con trabajos anteriores, y si en algún caso influyen
demasiado en las medidas, podremos minimizarlos no teniendo en cuenta
aquellos nodos cercanos al contorno.

\begin{figure}[h]\label{red_rombo}
\centering
\resizebox{275bp}{!}{\input{red_triangular-fig}}
\caption{Conectividad intrínseca de la red}
\end{figure}

Cada nodo es etiquetado mediante dos índices discretos $P=(i,j)\ i,j=1,L$
como se indica en la figura~\ref{red_rombo}. La posición en el espacio
tridimensional vendrá dada por el vector $\vec{r}_P$. Como energía libre discreta del sistema usamos
\begin{equation}
\mathcal{H}=\sum_{\langle PQ \rangle}
|\vec{r}_P-\vec{r}_Q|^2+\kappa\sum_{\langle ab \rangle}
|\vec{n}_a-\vec{n}_b|^2 
\end{equation}
donde con $\langle\rangle$ indicamos que la suma es sobre primeros vecinos,
nodos adyacentes. Estamos considerando únicamente interacciones de corto
alcance,  no inlcuimos ningún término de exclusión, ya que, es irrelevante en
la fase plana. Los  índices en letra minúscula representan las plaquetas, 
los triángulos de la red, y $\vec{n}_{a}$ el vector normal a la plaqueta. La
constante $\kappa$ es el módulo de rigidez y hemos escalado las posiciones de
forma que la constante elástica es unitaria.

%Colapso punto

\subsection{límite continuo}
\begin{figure}[h]
\centering
 \resizebox{\columnwidth}{!}{\input{base-hexagonal-fig}}
\caption{Base hexagono}
\end{figure}   
Consideramos un sistema de coordenadas ortogonal $\{ \vec{e}_1,\vec{e}_2\}$, son los vectores de la base del plano base. La expresión TAL podemos discretizarla
\begin{equation}
(\vec{r}_P-\vec{r}_Q)=a^2+2u_{\alpha\beta}x{\scriptstyle [q]}^{\alpha}x{\scriptstyle [q]}^{\beta}
\end{equation}
donde $u_{\alpha\beta}$ es la versión discretizada del tensor de
deformaciones. En este  sistema de coordenadas la métrica es
\begin{equation}
g_{\alpha\beta}=\vec{e}_{\alpha}\cdot\vec{e}_{\beta}=a^2\delta_{\alpha\beta}
\end{equation}
Podemos aproximar
\begin{equation}
|\vec{r}_P-\vec{r}_Q|=\sqrt{a^2+2u_{\alpha\beta}e^{\alpha}e^{\beta}}\simeq a + \frac{u_{\alpha\beta}}{a}e^{\alpha}e^{\beta}
\end{equation}



Sustituyendo en la parte elástica de la energía libre
\begin{equation}
\mathcal{H}_E\simeq \frac{1}{2}\sum_{P} \sum^6_{j=1} \left(\frac{u_{\alpha\beta}}{a}e^{\alpha}e^{\beta}\right)^2=\frac{1}{2a^2}\sum_{P} \sum^6_{j=1} u_{\alpha\beta}u^{\mu\nu}e^{\alpha}e^{\beta}e_{\mu}e_{\nu}
\end{equation}
Siendo el primer sumatorio sobre todos los nodos y el segundo sobre los
primeros vecinos. Teniendo en cuenta las relaciones
\begin{equation}
1=\frac{\delta^{\alpha\beta}\delta_{\alpha\beta}}{2}=\frac{\delta^{\mu\nu}\delta_{\mu\nu}}{2}
\end{equation}
Podemos escribir el término
\begin{equation}
e^{\alpha}e^{\beta}e_{\mu}e_{\nu}=\frac{1}{4}\delta^{\alpha\beta}\delta_{\mu\nu}\delta_{\alpha\beta}e^{\alpha}e^{\beta}\delta^{\mu\nu}e_{\mu}e_{\nu}=\frac{a^2}{4}\delta^{\alpha\beta}\delta_{\mu\nu}
\end{equation}
Y de forma análoga también podemos obtener
\begin{align}
e^{\alpha}e^{\beta}e_{\mu}e_{\nu}&=\frac{a^2}{4}\delta^{\alpha}_{\ \mu}\delta_{\beta}^{\
  \nu}\\
e^{\alpha}e^{\beta}e_{\mu}e_{\nu}&=\frac{a^2}{4}\delta^{\alpha}_{\ \nu}\delta_{\beta}^{\
  \mu}
\end{align}
Tomando la media de las últimas tres expresiones TAL
\begin{equation}
e^{\alpha}e^{\beta}e_{\mu}e_{\nu}=\frac{a^4}{12}(\delta^{\alpha\beta}\delta_{\mu\nu}+
\delta^{\alpha}_{\ \mu}\delta_{\beta}^{\ \nu}+
\delta^{\alpha}_{\ \nu}\delta_{\beta}^{\ \mu})
\end{equation}
Sustituyendo en TAL
\begin{equation}
\mathcal{H}_E\simeq\frac{a^2}{24}\sum_{P} \sum^6_{j=1}(\delta^{\alpha\beta}\delta_{\mu\nu}+\delta^{\alpha}_{\ \mu}\delta_{\beta}^{\ \nu}+
\delta^{\alpha}_{\ \nu}\delta_{\beta}^{\ \mu})
u_{\alpha\beta}u^{\mu\nu}=\frac{a^2}{4}\sum_{P}u_{\alpha}^{\ \alpha}u^{\alpha}_{\ \alpha}+2u_{\alpha\beta}u^{\alpha\beta}
\end{equation}
Donde en  la última igualdad hemos efectuado la suma sobre todos los vecinos.
Pasamos al límite continuo:
\begin{equation}
\frac{a^2}{4}\sum_{i=1}^L\sum_{j=1}^L\frac{\Delta i\Delta j}{\xi^2\frac{\sqrt{3}}{2}}\xi^2\,\frac{\sqrt{3}}{2}(u_{\alpha}^{\ \alpha}u^{\alpha}_{\ \alpha}+2u_{\alpha\beta}u^{\alpha\beta})
\end{equation}
Tomando el límite $N\rightarrow \infty$
elástica 
\begin{equation}
\mathcal{H}\simeq\int d^2\mathbf{x}\, \frac{\sqrt{3}}{4}\frac{a^2}{\xi^2}\left(\frac{1}{2}u_{\alpha}^{\ \alpha}u^{\alpha}_{\ \alpha}+u_{\alpha\beta}u^{\alpha\beta}\right)
\end{equation}
recuperamos la energía libre elástica con 
\begin{equation}
\lambda=\mu=\frac{\sqrt{3}}{4}\frac{a^2}{\xi^2}
\end{equation}
Nuestro modelo corresponde al límite $a\rightarrow 0$ y $\xi$ fijo.

Respecto al término de curvatura, podemos desarrollar $\vec{n}_{b}$ desde $a$
\begin{equation}
\vec{n}_b\simeq\vec{n}_a+\frac{\partial \vec{n_b}}{\partial x^{\alpha}}(x_b^{\alpha}-x_a^{\alpha})
\end{equation}
Sustituyendo en la energía libre
\begin{equation}
\mathcal{H}_C=\frac{1}{2}\kappa\sum_{\langle ab\rangle}\frac{\partial
  \vec{n_b}}{\partial x^{\alpha}}\cdot\frac{\partial \vec{n_b}}{\partial x^{\beta}} (x_b^{\alpha}-x_a^{\alpha})(x_b^{\beta}-x_a^{\beta})
\end{equation}
Podemos considerar que son triángulos equiláteros
\begin{equation}
g^{\alpha\beta}=(x_b^{\alpha}-x_a^{\alpha})(x_b^{\beta}-x_a^{\beta})\simeq
\sqrt{3} \delta^{\alpha\beta}
\end{equation}
Entonces
\begin{equation}
\mathcal{H}_C=\frac{\sqrt{3}}{2}\kappa\sum_{\langle ab\rangle}\frac{\partial
  \vec{n_b}}{\partial x_{\alpha}}\cdot\frac{\partial \vec{n_b}}{\partial x^{\alpha}}
\end{equation}
En el límite continuo, para $N\rightarrow \infty$
\begin{equation}
\mathcal{H}_C=\frac{1}{2}\hat{\kappa}\int d^2\mathbf{x}\ \frac{\partial
  \vec{n_b}}{\partial x_{\alpha}}\cdot\frac{\partial \vec{n_b}}{\partial x^{\alpha}}
\end{equation}
A partir de la expresión TAL
\begin{equation}
\frac{\partial \vec{n_b}}{\partial x_{\alpha}}\cdot\frac{\partial
  \vec{n_b}}{\partial x^{\alpha}}=(K_{\alpha\beta}\vec{t}^{\beta})\cdot(K_{\nu}^{\
  \alpha}\vec{t}^{\nu})=K_{\alpha\beta}g^{\beta\nu}K_{\nu}^{\ \alpha}=K_{\alpha\beta}K^{\alpha\beta}
\end{equation}
Entonces 
\begin{equation}
\mathcal{H}_C\simeq\frac{1}{2}\hat{\kappa}\int d^2\mathbf{x}\ K_{\alpha\beta}K^{\alpha\beta}
\end{equation}
Y recuperamos la ecuación TAL de la energía libre de Landua.

\section{Algoritmo de Metropolis}
Una determinada configuración $R$ de la membrana cristalina discreta 
corresponde a una lista de $N$ posiciones tridimensionales:
\begin{equation}
R=\{ \vec{r}_1,\vec{r}_2,\dots \vec{r}_N\}\,.
\end{equation}
Si tenemos $M$ configuraciones independientes que siguen una distribución
proporcional al factor $e^{-\mathcal{H}(\kappa)}$, A partir de las medidas
$A_1,A_2,\dots A_N$ en cada configuración de un observable $A$, podemos
estimar su valor medio para ese $\kappa$
\begin{equation}
\langle A \rangle_{\kappa}\simeq \frac{1}{M}\sum^M_{i=1} A_i\,.
\end{equation}
Para encontrar estas configuraciones empleamos un método de Monte Carlo, que
consiste en un camino aleatorio en el espacio de configuraciones, en donde, en
cada uno de los pasos al menos se modifica en la posición de un nodo. Este camino
aleatorio no necesita estar relacionado con ningún proceso físico, la eficacia
en la estimación de $\langle A \rangle_{\kappa}$ es la única premisa. Este
camino aleatorio debe, a partir de una configuración inicial y tras un número
de pasos (termalización), alcanzar la región de interés donde
las configuraciones son proporcionales a el factor $
e^{-\mathcal{H}(\kappa)}$. El inconveniente de este método es que $M$
configuraciones sucesivas en esta región de interés no son independientes, pues no se
diferencian lo suficiente entre sí, están correlacionadas, lo que implica que el
error de la estimación de $\langle A \rangle_{\kappa}$ con estas $M$
configuraciones no será del orden de $O(\sqrt{M})$ sino de $O(\sqrt{M_I})$,
donde $M_I$ es el número de configuraciones independientes.  

El camino aleatorio será una cadena de Markov: La configuración futura
depende solamente de la configuración actual, o dicho de otro modo, nuestro
\textit{caminante} decide a donde ir en base a su posición actual. Entonces,
el proceso está completamente descrito mediante un operador 
de transición $W(R,S)$, que determina la probabilidad
alcanzar una configuración $S$ en el tiempo $t_0+1$ desde una
configuración $R$ en el tiempo $t_0$. Algunas propiedades de este
operador son 
\begin{itemize}
\item $W(R,S)\geq 0$ para todas las configuraciones
  $R$ y $S$. Además $1=\int
  dS\,W(R,S) $, ya que es una probabilidad.
\item La probabilidad de alcanzar la configuración $U$ desde
  $R$ en $l$ pasos es el producto de los operadores intermedios para cada paso 
  \begin{multline}
    P(U_{l+m}|R_m)=\int
  dR_{l+m-1}\dots dR_{m+2}
  dR_{m+1}\\W(U,R_{l+m-1})\dots
  W(R_{m+2},R_{m+1}) W(R_{m+1},R_{m})=W_l(\mathcal{U},R)
  \end{multline}
  también $W_l(\mathcal{U},R) \geq 0$ y $1=\int
  dS\, W_l(\mathcal{U},R)$.
\item Si en el tiempo $t_0$ la probabilidad densidad de probabilidad de
  encontrar a nuestro caminante en una región $dR$ es $\rho_{t_0}(R)dR$, en el tiempo $t_0+t$ será
\begin{equation}
\rho_{t_0+t}(S)=\int
  dR\, W_l(R,S)\rho_s(R)
\end{equation}
\end{itemize}
Estas propiedades son generales para cualquier cadena de Markov. Las
siguientes propiedades son necesarias para construir nuestro método de Monte
Carlo:
\begin{itemize}
\item La condición de balance:
  \begin{equation}\label{ecuacion_balance}
    \frac{e^{-\mathcal{H}(R,\kappa)}}{\mathcal{Z}}=\int
    dS\, W(R,S)\frac{e^{-\mathcal{H}(S,\kappa)}}{\mathcal{Z}}
  \end{equation}
  Si la posición de un conjunto de caminantes está distribuida, en un tiempo
  $t_0$, de acuerdo a $\frac{e^{-\mathcal{H}(S,\kappa)}}{\mathcal{Z}}$,en el
  tiempo $t_0+1$, las posiciones cambiarán, resultando una nueva configuración
  $S$, pero seguirán distribuyéndose de acuerdo al factor de Boltzmann $e^{-\mathcal{H}(R,\kappa)}$.
\item Todas las configuraciones son accesibles: Existe un entero $m_t$ tal que
  $W_{m_t}(R,S)>0$ para toda configuración $R$ y $S$ y todo $t>m_t$. 
\end{itemize}

Es posible demostrar que un operador de transición que satisfaga las dos
últimas propiedades cumplirá que
\begin{equation}
\lim_{t\rightarrow \infty}W_t(R,S)=\frac{e^{-\mathcal{H}(R,\kappa)}}{\mathcal{Z}}
\end{equation}
Este teorema nos asegura que nuestro caminante siempre alcanzará la región de
interés, no importa donde comience siempre se alcanzará la región donde las configuraciones siguen una
distribución de acuerdo al factor de Boltzamann. 

Podemos descomponer el operador de transición $W(R,S)$ en operadores
individuales $w^i(R,S)$, en los que sólo varía la posición $\vec{r}_i$. El
operador de transición total corresponderá a aplicar sucesivamente los $N$
operadores individuales. Si el orden de aplicación es siempre el mismo tenemos
\begin{equation*}
w(R,S)=\Pi^{N}_{i=1}w^i(R,S)
\end{equation*}
Si los operadores de transición cumplen la condición de
balance \label{ecuacion_balance}, el operador total también la cumplirá, dado
que es un producto de operadores. Esto lo podemos conseguir exigiendo que los operadores
$w^i(R,S)$ cumplan
\begin{equation}\label{balance_detallado}
w^i(R,S)\frac{e^{-\mathcal{H}(R,\kappa)}}{\mathcal{Z}}=w^i(S,R)\frac{e^{-\mathcal{H}(S,\kappa)}}{\mathcal{Z}}\Rightarrow \frac{w^i(R,S)}{w^i(S,R)}=e^{-[\mathcal{H}(S,\kappa)+\mathcal{H}(R,\kappa)]}
\end{equation}
dado que $1=\int dS w^i(R,S)$. Una forma de construir estos operadores de
transición individuales es el algoritmo de Metropolis, que consiste en
descomponer el operador de transición individual en 
 \begin{equation*}
 w^i(R,S)=A_{RS}\omega^i(R,S)
\end{equation*}
donde $A_{RS}$ es un factor simétrico, la probabilidad de alcanzar la
configuración $S$ desde $R$, es la misma que alcanzar $R$ desde $S$; y
$\omega^i(R,S)$ es un peso elegido como función del cociente entre los
facotres de Boltzmann
\begin{equation*}
\omega^i(R,S)=g(e^{-[\mathcal{H}(S,\kappa)+\mathcal{H}(R,\kappa)]}).
\end{equation*}
La condición \label{balance_detallado} será satisfecha si la función $g$
cumple
\begin{equation}
\frac{g(z)}{g(1/z)}=z.
\end{equation}
El algoritmo de Metropolis utiliza $g_M(z)=min\{1,z\}$ y lo hemos implementado de
 la siguiente forma:
\begin{itemize}
\item A partir de la configuración $R$ de la membrana proponemos como
  siguiente configuración $S_i$, que únicamente se diferencia de $R$ en la
  posición de un punto que ha sido modificada en un vector $\vec{\epsilon}$,
  elegido aleatoriamente y uniformemente en un cubo de volumen 
  $(2\varepsilon)^3$. Estamos proponiendo un cambio de forma simétrica, a
  partir de $S$ también es posible proponer $S$ como siguiente configuración
  con la misma probabilidad.
\item Calculamos la diferencia de energía entre las dos configuraciones
  $\Delta\mathcal{H}=\mathcal{H}_S-\mathcal{H}_S$:
  \begin{itemize}
    \item Si es negativa aceptamos la configuración $S$.
    \item Si es positiva, aceptamos la configuración con una probabilidad $e^{-\Delta\mathcal{H}}$.
  \end{itemize}
\end{itemize}

Un barrido de corresponde a aplicar los pasos anteriores de forma secuencial a
todos los $N$ puntos de la membrana. El valor de $\varepsilon$ se ajusta de
modo que en cada barrido el número de cambios aceptados sea la mitad del total
de puntos que componen la superficie. 
 
\subsection{Método de Jacknife}

Para el cálculo de los errores, en las estimaciones de los observables,
utilizamos el método del jacknife \cite{Juan:tesis}, que tiene en cuenta la
falta de independencia de las medidas. Consideremos los valores
${Q_i;i=1,\dots M}$ que un observable $Q$ toma en una evolución de Monte
Carlo. La estimación del valor medio será
\begin{equation}
\bar{Q}=\frac{1}{M}\sum^M_{i=1}Q_i.
\end{equation}
Para estimar el error se divide el conjunto de $M$ valores en $K$ bloques
iguales, tendrán $m=M/K$ datos cada uno. Definimos $\bar{Q}^m_k \, \{k=1,\dots
M\}$ como el promedio de los datos después de eliminar el bloque $k$-ésimo
\begin{equation}
\bar{Q}_k^m=\frac{1}{M-m}\left(\sum^{m(k-1)}_{i=1}Q_i+\sum^{M}_{i=mk+1}Q_i\right).
\end{equation}
La estimación del error para el tamaño de bloque $m$ es
\begin{equation}
\delta\bar{Q}_m=\sqrt{\frac{K-1}{K}\sum^K_{k=1}(\bar{Q}^m_k-\bar{Q})}.
\end{equation}
Esta última expresión da una estimación no sesgada del error, para comprobarlo
basta con tomar la esperanza $\langle \rangle$ a ambos lados de la ecuación
\begin{equation}
\langle (\delta\bar{Q}_n)^2\rangle=\sigma^2(Q)\frac{1}{N} 
\end{equation}
Si el tamaño del bloque, $m$, es muy pequeño comparada con la longitud de
correlación entre medidas, el estimador $\delta\bar{Q}_m$ aumentará en un
factor $\sqrt{2}$ cada vez que doblamos el tamaño del bloque. Este crecimiento
cesará cuando el tamaño del bloque sea mayor que la correlación entre
medidas. A partir de aquí, el valor del error se mantiene constante hasta que
el número de bloques es demasiado pequeño (2 o 3), que vuelve a crecer con un
factor $\sqrt{2}$.

\section{Observables}

\subsection{Valores críticos: Densidad espectral}

\begin{itemize}
\item El máximo del observable, que ocurre a una temperatura crítica
  $\kappa_c(T)$, la cual es modificada respecto al tamaño infinito del sistema

$$ T_cL -T_c= L^{-1/\nu}$$
\item El máximo del observable de estado finito es dado por la relación de
  escala
$$ C_v(T_c(L),L^-1)=L^{\alpha/nu}$$
\end{itemize}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "TFM"
%%% End: 
